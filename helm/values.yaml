daskhub:
  jupyterhub:
    prePuller:
      # only pre-pull the default image (Python)
      pullProfileListImages: false

    scheduling:
      podPriority:
        enabled: true
      userScheduler:
        enabled: true
      corePods:
        nodeAffinity:
          matchNodePurpose: "require"

    hub:
      baseUrl: "/compute/"
      image:
        name: jupyterhub/k8s-hub
        tag: "1.0.1"

      config:
        JupyterHub:
          admin_access: false
          admin_users:
            - alex@developmentseed.org
            - alexbush@developmentseed.org

          authenticator_class: generic-oauth
        GenericOAuthenticator:
          client_id: "KsF8JRehUgWVe9X2vLSu4u2p13Xm89np"
          oauth_callback_url: "https://dep-pccompute.westeurope.cloudapp.azure.com/compute/hub/oauth_callback"
          username_key: "email"
          authorize_url: "https://dev-3bb54g9o.us.auth0.com/authorize"
          token_url: "https://dev-3bb54g9o.us.auth0.com/oauth/token"
          userdata_url: "https://dev-3bb54g9o.us.auth0.com/userinfo"
          scope:
            - openid
            - name
            - profile
            - email

      extraEnv:
        # This service principal has read permission on Azure API management.
        AZURE_TENANT_ID: "72f988bf-86f1-41af-91ab-2d7cd011db47"
        AZURE_CLIENT_ID: "185e2bd5-f11e-490c-8849-7889951120b4"
        # AZURE_CLIENT_SECRET, PC_ID_TOKEN, APPLICATIONINSIGHTS_CONNECTION_STRING are set via terraform

      networkPolicy:
        # Disable hub network Policy, so that the dask gateway server API can
        # reach the hub directly
        # https://github.com/dask/helm-chart/issues/142
        enabled: false

      extraFiles:
        jupyterhub_opencensus_monitor:
          mountPath: /usr/local/jupyterhub_opencensus_monitor.py
          # TODO(https://github.com/hashicorp/terraform-provider-helm/issues/628): use set-file
          # Using jupyterhub_opencensus_monitor.yaml for now.

      services:
        opencensus-monitoring:
          command:
            - python3
            - /usr/local/jupyterhub_opencensus_monitor.py
          admin: true
        kbatch:
          # api_token and URL are set by terraform
          admin: true

      # Volumes for customizing the JupyterHub UI
      # https://discourse.jupyter.org/t/customizing-jupyterhub-on-kubernetes/1769/4
      extraVolumes:
        - name: hub-templates
          configMap:
            name: hub-templates
        - name: hub-external
          configMap:
            name: hub-external

      extraVolumeMounts:
        - name: hub-templates
          mountPath: /etc/jupyterhub/templates
        - name: hub-external
          mountPath: /usr/local/share/jupyterhub/static/external

      extraConfig:
        mylabels: |
          c.KubeSpawner.extra_labels = {}
        kubespawner: |
          c.KubeSpawner.start_timeout = 15 * 60  # 15 minutes
        01-add-dask-gateway-values: |
          # The daskhub helm chart doesn't correctly handle hub.baseUrl.
          # DASK_GATEWAY__PUBLIC_ADDRESS set via terraform
          c.KubeSpawner.environment["DASK_GATEWAY__ADDRESS"] = "http://proxy-http:8000/compute/services/dask-gateway/"
          c.KubeSpawner.environment["DASK_GATEWAY__PUBLIC_ADDRESS"] = "https://${jupyterhub_host}/compute/services/dask-gateway/"
        templates: |
          c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")

    proxy:
      https:
        enabled: true
        letsencrypt:
          contactEmail: "alexbush@developmentseed.org"

    singleuser:
      # These limits match the "large" profiles, so that a user requesting large will be successfully scheduled.
      # The user scheduler doesn't evict multiple placeholders.
      memory:
        limit: "30G"
        guarantee: "30G"
      cpu:
        limit: 6.0
        guarantee: 6.0

      storage:
        capacity: "15Gi"
        extraVolumes:
          - name: common
            persistentVolumeClaim:
              claimName: common-file-pvc

          - name: user-etc-singleuser
            configMap:
              name: user-etc-singleuser

          # Workaround small /dev/shm issue.
          # https://github.com/pangeo-data/pangeo-docker-images/issues/258
          # https://stackoverflow.com/questions/46085748/define-size-for-dev-shm-on-container-engine/46434614#46434614
          # This can be fixed upstream in planetary-computer-containers once the docker GitHub action
          # is updated to support setting shm-size.
          # https://github.com/docker/build-push-action/issues/263
          - name: dshm
            emptyDir:
              medium: Memory

          # Read-only file share for ML datasets
          - name: driven-data
            azureFile:
              secretName: driven-data-file-share
              secretNamespace: ${namespace}
              shareName: driven-data
              readOnly: true

        extraVolumeMounts:
          - name: user-etc-singleuser
            mountPath: /etc/singleuser

          - name: dshm
            mountPath: /dev/shm

          - name: common
            mountPath: /home/jovyan/common

          - name: driven-data
            mountPath: /driven-data/

      extraEnv:
        DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: '{JUPYTER_IMAGE_SPEC}'
        DASK_DISTRIBUTED__DASHBOARD__LINK: '/user/{JUPYTERHUB_USER}/proxy/{port}/status'
        DASK_LABEXTENSION__FACTORY__MODULE: 'dask_gateway'
        DASK_LABEXTENSION__FACTORY__CLASS: 'GatewayCluster'
        NVIDIA_DRIVER_CAPABILITIES: 'compute,utility'
        # GDAL / Rasterio environment variables for performance
        GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
        GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"

      lifecycleHooks:
        postStart:
          exec:
            command:
              - "bash"
              - "/etc/singleuser/k8s-lifecycle-hook-post-start.sh"


  dask-gateway:
    gateway:
      prefix: "/compute/services/dask-gateway"
      auth:
        jupyterhub:
          apiToken: "{{ tf.jupyterhub_dask_gateway_token }}"
          apiUrl: http://proxy-http:8000/compute/hub/api
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hub.jupyter.org/node-purpose
                operator: In
                values:
                - core

      backend:
        scheduler:
          cpu:
            requests: 1.0
            limit: 2.0
          memory:
            requests: 8G
            limit: 10G
          extraPodConfig:
            tolerations:
              - key: 'hub.jupyter.org_dedicated'
                operator: 'Equal'
                value: 'user'
                effect: 'NoSchedule'
            affinity:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                      - key: hub.jupyter.org/node-purpose 
                        operator: In
                        values:
                          - core
                          - user

        worker:
          extraPodConfig:
            tolerations:
              - key: "k8s.dask.org/dedicated"
                operator: "Equal"
                value: "worker"
                effect: "NoSchedule"
              - key: "k8s.dask.org_dedicated"
                operator: "Equal"
                value: "worker"
                effect: "NoSchedule"
              - key: "kubernetes.azure.com/scalesetpriority"
                operator: "Equal"
                value: "spot"
                effect: "NoSchedule"

      extraConfig:
        01-idle: |
          c.KubeClusterConfig.idle_timeout = 20 * 60  # seconds
          c.KubeClusterConfig.cluster_max_cores = 400  # 50 nodes @ 8 workers / node, 1 core / worker
          c.KubeClusterConfig.cluster_max_memory = "3200 G"  # 8 GiB / core
          c.KubeClusterConfig.cluster_max_workers = 400  # 1 core, 8 GiB / worker

        02-optionHandler: |
            from dask_gateway_server.options import Options, Float, String, Mapping, Bool

            def cluster_options(user):
                def option_handler(options):
                    if ":" not in options.image:
                        raise ValueError("When specifying an image you must also provide a tag")

                    def escape(username):
                        import string

                        safe_chars = set(string.ascii_lowercase + string.digits)
                        chars = []
                        for char in username:
                            if char in safe_chars:
                                chars.append(char.lower())
                            else:
                                chars.append(".")
                        return "".join(chars)

                    extra_annotations = {
                        "hub.jupyter.org/username": user.name,
                    }
                    extra_labels = {
                        "hub.jupyter.org/username": escape(user.name),
                    }
                    # Maybe add a GPU request
                    worker_extra_pod_config = {
                        "tolerations": [
                            {
                                "key": "kubernetes.azure.com/scalesetpriority",
                                "operator": "Equal",
                                "value": "spot",
                                "effect": "NoSchedule",
                            },
                            {
                                "key": "k8s.dask.org_dedicated",
                                "operator": "Equal",
                                "value": "worker",
                                "effect": "NoSchedule",
                            },
                        ]
                    }
                    if options.gpu:
                        node_affinity = {
                            "key": "pc.microsoft.com/workerkind",
                            "operator": "In",
                            "values": ["gpu"],
                        }

                        worker_extra_container_config = {
                            "resources": {
                                "limits": {
                                    "nvidia.com/gpu": 1,
                                },
                            },
                        }
                        worker_extra_pod_config["tolerations"].append(
                            {
                                "key": "nvidia.com/gpu",
                                "operator": "Equal",
                                "value": "present",
                                "effect": "NoSchedule",
                            }
                        )
                        options.environment["NVIDIA_DRIVER_CAPABILITIES"] = 'compute,utility'
                    else:
                        worker_extra_container_config = {}
                        node_affinity = {
                            "key": "pc.microsoft.com/workerkind",
                            "operator": "In",
                            "values": ["cpu"],
                        }

                    # Prevents worker pods from using the core pool.
                    dask_worker_affinity = {
                        "key": "k8s.dask.org/dedicated",
                        "operator": "In",
                        "values": ["worker"],
                    }
                    worker_extra_pod_config["affinity"] = {
                        "nodeAffinity": {
                            "requiredDuringSchedulingIgnoredDuringExecution": {
                                "nodeSelectorTerms": [
                                    {"matchExpressions": [node_affinity, dask_worker_affinity]},
                                ],
                            },
                        },
                    }

                    # We multiply the requests by 0.95 to ensure that that they
                    # pack well onto nodes. Kubernetes reserves a small fraction
                    # of the memory / CPU for itself, so the common situation of
                    # a node with 4 cores and a user requesting 4 cores means
                    # we request just over half of the *allocatable* CPU, and so
                    # we can't pack more than 1 worker on that node.
                    # On GCP, the kubernetes requests are ~12% of the CPU.
                    return {
                        "worker_cores": 0.9 * options.worker_cores,
                        "worker_cores_limit": options.worker_cores,
                        "worker_memory": "%fG" % (0.88 * options.worker_memory),
                        "worker_memory_limit": "%fG" % options.worker_memory,
                        "image": options.image,
                        "scheduler_extra_pod_annotations": extra_annotations,
                        "worker_extra_pod_annotations": extra_annotations,
                        "scheduler_extra_pod_labels": extra_labels,
                        "worker_extra_pod_labels": extra_labels,
                        "worker_extra_container_config": worker_extra_container_config,
                        "environment": options.environment,
                        "worker_extra_pod_config": worker_extra_pod_config,
                        "gpu": options.gpu,
                    }

                default_env = {
                    "GDAL_DISABLE_READDIR_ON_OPEN": "EMPTY_DIR",
                    "GDAL_HTTP_MERGE_CONSECUTIVE_RANGES": "YES"
                }
                return Options(
                    Float("worker_cores", 1, min=0.1, max=8, label="Worker Cores"),
                    Float("worker_memory", 8, min=1, max=64, label="Worker Memory (GiB)"),
                    String("image", default="pangeo/pangeo-notebook:latest", label="Image"),
                    Bool("gpu", default=False, label="GPU"),
                    Mapping("environment", default=default_env, label="Environment Variables"),
                    handler=option_handler,
                )
            c.Backend.cluster_options = cluster_options
    traefik:
      service:
        type: LoadBalancer
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: hub.jupyter.org/node-purpose
                operator: In
                values:
                - core
